{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeli\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210111). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e92862bdb227>:39: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D,GlobalAveragePooling2D,Dense,Softmax,Flatten,MaxPooling2D,Dropout,Activation, Lambda, concatenate,BatchNormalization\n",
    "from keras.datasets import cifar100, cifar10, mnist\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import kullback_leibler_divergence\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "import random\n",
    "import gc\n",
    "#import seaborn as sns\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "#import os\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, Input, Add, BatchNormalization, Activation\n",
    "from keras.activations import relu, softmax\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Load the train and test data splits\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_resize_crop(image):\n",
    "    start_x=random.randint(1,32)\n",
    "    start_y=random.randint(1,32)\n",
    "    height=random.randint(1,32)\n",
    "    length=random.randint(1,32)\n",
    "\n",
    "    for i in range(start_x, min(31, start_x+length)):\n",
    "        for j in range(start_y, min(31, start_y+height)):\n",
    "            image[i][j]=0\n",
    "    return image\n",
    "    \n",
    "    \n",
    "\n",
    "def color_jitter(x):\n",
    "    s=1.0\n",
    "    x = tf.image.random_brightness(x, max_delta=0.8*s)\n",
    "    x = tf.image.random_contrast(x, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
    "    x = tf.image.random_saturation(x, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
    "    x = tf.image.random_hue(x, max_delta=0.2 * s)\n",
    "    # x = tf.clip_by_value(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def color_drop(x):\n",
    "    x = tf.image.rgb_to_grayscale(x)\n",
    "    x = tf.tile(x, [1, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_augmentation_2(image):\n",
    "\n",
    "    number = tf.random.uniform(shape=[], minval=1, maxval=100)\n",
    "    if (number<=40):\n",
    "        image = random_resize_crop(image)\n",
    "    elif (number<=70):\n",
    "        image= color_jitter(image)\n",
    "    else:\n",
    "        image= color_drop(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive learning__ x_train_aug: (50000, 32, 32, 3)     y_train: (50000, 1)\n",
      "Contrastive evaluate__ x_test_aug: (10000, 32, 32, 3)     y_test: (10000, 1)\n",
      "\n",
      "\n",
      "Supervised learning__ x_train_shf: (50000, 32, 32, 3)     y_train_shf: (50000, 10)\n",
      "Supervised evaluate__ x_test: (10000, 32, 32, 3)     y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "## data\n",
    "import random\n",
    "\n",
    "x_train_aug=np.zeros_like(x_train)\n",
    "y_train_aug= y_train.copy()\n",
    "y_test_aug= y_test.copy()\n",
    "x_test_aug=np.zeros_like(x_test)\n",
    "\n",
    "\n",
    "# Train images process\n",
    "for i in range(x_train.shape[0]):\n",
    "    x= x_train[i].copy()\n",
    "    x= data_augmentation_2(x)              #plt.imshow(x)\n",
    "    x_train_aug[i]=x\n",
    "x_train_aug= tf.image.convert_image_dtype(x_train_aug, dtype=tf.float32, saturate=False, name=None)\n",
    "\n",
    "\n",
    "# Test images process\n",
    "for i in range(x_test.shape[0]):\n",
    "    x= x_test[i].copy()\n",
    "    x= data_augmentation_2(x)              #plt.imshow(x)\n",
    "    x_test_aug[i]=x\n",
    "x_test_aug= tf.image.convert_image_dtype(x_test_aug, dtype=tf.float32, saturate=False, name=None)\n",
    "\n",
    "\n",
    "\n",
    "n_train = x_train_aug.shape[0]\n",
    "shuffle_idx = np.arange(n_train)\n",
    "np.random.shuffle(shuffle_idx)\n",
    "\n",
    "x_train_shf = x_train[shuffle_idx][:n_train]\n",
    "y_train_shf = y_train[shuffle_idx][:n_train]\n",
    "y_train_shf= to_categorical(y_train_shf, num_classes)\n",
    "y_test= to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Contrastive learning__ x_train_aug: {x_train_aug.shape}     y_train: {y_train_aug.shape}')\n",
    "print(f'Contrastive evaluate__ x_test_aug: {x_test_aug.shape}     y_test: {y_test_aug.shape}')\n",
    "print('\\n')\n",
    "print(f'Supervised learning__ x_train_shf: {x_train_shf.shape}     y_train_shf: {y_train_shf.shape}')\n",
    "print(f'Supervised evaluate__ x_test: {x_test.shape}     y_test: {y_test.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "\n",
    "learning_rate = 0.0001   #0.001\n",
    "batch_size = 100\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs_con = 5\n",
    "num_epochs_sup = 5\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "#contrastive: Projection\n",
    "#Supervised: Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder():\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    x= Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Dropout(0.3)(x)\n",
    "    x= Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x= Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Dropout(0.4)(x)\n",
    "    x= Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features= Flatten()(features)\n",
    "    features= layers.Dropout(dropout_rate)(features)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.SGD(learning_rate= learning_rate, decay= 1e-6, momentum=0.9, nesterov=True),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        #metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features= Flatten()(features)\n",
    "    features= layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_euclidean(A):\n",
    "    # Euclidean pdist\n",
    "    r = tf.reduce_sum(A*A, 1)\n",
    "\n",
    "    # turn r into column vector\n",
    "    r = tf.reshape(r, [-1, 1])\n",
    "    D = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n",
    "\n",
    "    return tf.sqrt(D)\n",
    "\n",
    "\n",
    "def square_to_vec(D):\n",
    "    '''Convert a squared form pdist matrix to vector form.\n",
    "    '''\n",
    "    n = D.shape[0]\n",
    "    triu_idx = np.triu_indices(n, k=1)\n",
    "    d_vec = tf.gather_nd(D, list(zip(triu_idx[0], triu_idx[1])))\n",
    "    return d_vec\n",
    "\n",
    "\n",
    "def get_contrast_batch_labels(y):\n",
    "    '''\n",
    "    Make contrast labels by taking all the pairwise in y\n",
    "    y: tensor with shape: (batch_size, )\n",
    "    returns:\n",
    "        tensor with shape: (batch_size * (batch_size-1) // 2, )\n",
    "    '''\n",
    "    y_col_vec = tf.reshape(tf.cast(y, tf.float32), [-1, 1])\n",
    "    D_y = pdist_euclidean(y_col_vec)\n",
    "    d_y = square_to_vec(D_y)\n",
    "    y_contrasts = tf.cast(d_y == 0, tf.int32)\n",
    "    return y_contrasts\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "class max_margin_contrastive_loss_euclidean(keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, name=None):\n",
    "        super(max_margin_contrastive_loss_euclidean, self).__init__(name=name)\n",
    "        self.margin=margin\n",
    "    def __call__(self,labels, feature_vectors, sample_weight=None):\n",
    "        feature_vectors=tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        D= pdist_euclidean(feature_vectors)\n",
    "        d_vec= square_to_vec(D)\n",
    "        y_contrasts= get_contrast_batch_labels(labels)\n",
    "        loss= tfa.losses.contrastive_loss(y_contrasts, d_vec, margin=margin)\n",
    "        loss= tf.reduce_mean(loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class max_margin_contrastive_loss_cosine(keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, name=None):\n",
    "        super(max_margin_contrastive_loss_cosine, self).__init__(name=name)\n",
    "        self.margin=margin\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred=tf.math.l2_normalize(y_pred, axis=1)\n",
    "        D= 1- tf.matmul(y_pred, y_pred, transpose_b=True)\n",
    "        d_vec= square_to_vec(D)\n",
    "        y_contrasts= get_contrast_batch_labels(y_true)\n",
    "        loss= tfa.losses.contrastive_loss(y_contrasts, d_vec, margin=margin)\n",
    "        loss= tf.reduce_mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class multiclass_npairs_loss(keras.losses.Loss):   #SupervisedContrastiveLoss랑 똑같은데 temp로 나눈거\n",
    "    def __init__(self, name=None):\n",
    "        super(multiclass_npairs_loss, self).__init__(name=name)\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred=tf.math.l2_normalize(y_pred, axis=1)\n",
    "        S= tf.matmul(y_pred, y_pred, transpose_b= True)\n",
    "        #loss= tfa.losses.npairs_loss(y_true, S)\n",
    "        loss= tfa.losses.npairs_loss(tf.squeeze(y_true), S)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class triplet_loss(keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, kind='hard', name=None):\n",
    "        super(triplet_loss, self).__init__(name=name)\n",
    "        self.margin=margin\n",
    "        self.kind= kind\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        if kind=='hard':\n",
    "            loss= tfa.losses.triplet_hard_loss(y_true, y_pred, margin=margin, soft=False)\n",
    "        elif kind=='soft':\n",
    "            loss= tfa.losses.triplet_hard_loss(y_true, y_pred, margin=margin, soft=True)\n",
    "        elif kind=='semihard':\n",
    "            loss= tfa.losses.triplet_semihard_loss(y_true, y_pred, margin=margin)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class supervised_nt_xent_loss(keras.losses.Loss):\n",
    "    def __init__(self,temperature=0.5, name=None):\n",
    "        super(supervised_nt_xent_loss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self,  labels, feature_vectors, sample_weight=None):\n",
    "        base_temperature = 1\n",
    "\n",
    "        feature_vectors=tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        batch_size= tf.shape(feature_vectors)[0]\n",
    "        contrast_count=1\n",
    "        anchor_count=contrast_count\n",
    "        labels= tf.expand_dims(labels,-1)\n",
    "\n",
    "        # mask는 [batch_size, batch_size] dim이고, mask[i,j]=1 if i랑 j의 class 같으면\n",
    "        mask= tf.cast(tf.equal(labels, tf.transpose(labels)), tf.float32)\n",
    "        mask= tf.reshape(mask, (mask.shape[0], mask.shape[0]))#\n",
    "        anchor_dot_contrast= tf.divide(tf.matmul(feature_vectors, tf.transpose(feature_vectors)), self.temperature)\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max= tf.reduce_max(anchor_dot_contrast, axis=1, keepdims=True)\n",
    "        logits= anchor_dot_contrast - logits_max\n",
    "\n",
    "        # tile mask \n",
    "        logits_mask= tf.ones_like(mask) - tf.eye(batch_size)  #eye는 가운데 eigen\n",
    "        mask= mask*logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits= tf.exp(logits)*logits_mask\n",
    "        log_prob= logits - tf.math.log(tf.reduce_sum(exp_logits, axis=1, keepdims=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive \n",
    "        mask_sum = tf.reduce_sum(mask, axis=1)\n",
    "        mean_log_prob_pos= tf.reduce_sum(mask*log_prob, axis=1)[mask_sum>0] / mask_sum[mask_sum>0]\n",
    "\n",
    "        #loss\n",
    "        loss= -(self.temperature/base_temperature)*mean_log_prob_pos\n",
    "        loss= tf.reduce_mean(loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "model_17 (Functional)        (None, 8, 8, 128)         261696    \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 128)               1048704   \n",
      "=================================================================\n",
      "Total params: 1,310,400\n",
      "Trainable params: 1,309,632\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 20s 39ms/step - loss: 1.0423\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.9789\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.9448\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.9152\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.8909\n"
     ]
    }
   ],
   "source": [
    "# Contrastive learning\n",
    "\n",
    "encoder = create_encoder()\n",
    "margin=1.0\n",
    "temperature=0.2\n",
    "kind='hard'\n",
    "#custom_loss=SupervisedContrastiveLoss(temperature)      #낫밷 .adam opti가 더 나음 sgd보다\n",
    "#custom_loss=max_margin_contrastive_loss_euclidean(margin)  #nan  #l2norm 하니까 나옴 근데 10%\n",
    "#custom_loss=max_margin_contrastive_loss_cosine(margin)  #supervised 가 10-> batchsize 줄여서 25\n",
    "#custom_loss=multiclass_npairs_loss()   #ㄱㅊ\n",
    "#custom_loss=triplet_loss(margin, kind)   #supervised 가 14%\n",
    "custom_loss=supervised_nt_xent_loss(temperature)    #nan   #l2norm 하니까 나옴  #lr 0.0001하니까 43퍼나옴\n",
    "\n",
    "\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=custom_loss,\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history_p = encoder_with_projection_head.fit(\n",
    "    x=x_train_aug, y=y_train_aug, batch_size=batch_size, epochs=num_epochs_con)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "cifar-encoder_with_projectio (None, 128)               1310400   \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,381,578\n",
      "Trainable params: 71,178\n",
      "Non-trainable params: 1,310,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1526.4631 - accuracy: 0.1899 0s - loss: 1529.0929 - accuracy: 0.18\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 753.7883 - accuracy: 0.3074\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 511.0095 - accuracy: 0.3199\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 340.8167 - accuracy: 0.33142s\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 232.7413 - accuracy: 0.3260\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 13.7611 - accuracy: 0.4345\n",
      "Test accuracy: 43.45%\n"
     ]
    }
   ],
   "source": [
    "# Supervised Learning\n",
    "\n",
    "classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "classifier.summary()\n",
    "\n",
    "\n",
    "history_c = classifier.fit(x=x_train_shf, y=y_train_shf, batch_size=batch_size, epochs=num_epochs_sup)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "model_8 (Functional)         (None, 8, 8, 128)         261696    \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 4,523,466\n",
      "Trainable params: 4,522,698\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.9730 - categorical_accuracy: 0.1979\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.8581 - categorical_accuracy: 0.3808\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.5896 - categorical_accuracy: 0.4866\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.4124 - categorical_accuracy: 0.5545\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.2864 - categorical_accuracy: 0.5986\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.2001 - categorical_accuracy: 0.6263\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.1137 - categorical_accuracy: 0.6570\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.0477 - categorical_accuracy: 0.6844\n",
      "Epoch 9/10\n",
      " 21/189 [==>...........................] - ETA: 9s - loss: 1.0071 - categorical_accuracy: 0.7017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-01811dedd5f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train_shf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_shf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs_con\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnum_epochs_sup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1133\u001b[0m                 _r=1):\n\u001b[0;32m   1134\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    838\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstep_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m       outputs = reduce_per_replica(\n\u001b[0;32m    832\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   1262\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 1263\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2733\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2734\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2735\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2737\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3423\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3424\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3426\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m         \u001b[1;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mreplica_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_strategy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreplica_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m       \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m       \u001b[1;31m# TODO(psv): Test distribution of metrics using different distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hlnam\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3928\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3929\u001b[1;33m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[0;32m   3930\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3931\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# No contrastive, just supervised\n",
    "\n",
    "encoder = create_encoder()\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()\n",
    "\n",
    "history = classifier.fit(x=x_train_shf, y=y_train_shf, batch_size=batch_size, epochs=num_epochs_con+num_epochs_sup)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
